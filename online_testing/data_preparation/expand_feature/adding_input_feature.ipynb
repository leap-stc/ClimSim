{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a77947b-4028-4ee1-8e92-896d1a3104a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# expand each sample .nc file with additional featuers such as previous steps' information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae457a-c126-412c-aa69-abb2fef42b26",
   "metadata": {},
   "source": [
    "## Load modules, determine available cpus, create list of input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d1b7773-9960-43bd-bcfa-3fdc12749475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from climsim_adding_input import process_one_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5857b68-1a73-4834-aed6-833d3b3d2089",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available CPUs: 256\n"
     ]
    }
   ],
   "source": [
    "# Get the number of available CPUs\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "print(f\"Number of available CPUs: {num_cpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f405739-e4cf-4ad7-b5ef-3c526bb530ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210240"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = \"/global/homes/z/zeyuanhu/hugging/E3SM-MMF_ne4/train\"\n",
    "nc_files_in = sorted(glob.glob(os.path.join(base_dir, '**/E3SM-MMF.mli.*.nc'), recursive=True))\n",
    "len(nc_files_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0351edf-c8fc-4220-8930-3b497974802b",
   "metadata": {},
   "source": [
    "## Create new nc files that contains additional input features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138dc7b8-cdbd-4fd8-be66-fce146a74706",
   "metadata": {},
   "source": [
    "Below we will use multiprocessing to speed up the data processing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845358ea-200a-4f4b-93f6-58c21d02cdb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds0 = xr.open_dataset('../../grid_info/ClimSim_low-res_grid-info.nc')\n",
    "lat = ds0['lat']\n",
    "lon = ds0['lon']\n",
    "\n",
    "mp.set_start_method('spawn')\n",
    "if __name__ == '__main__':\n",
    "    # Determine the number of processes based on system's capabilities or your preference\n",
    "    num_processes = mp.cpu_count()  # You can adjust this to a fixed number if preferred\n",
    "\n",
    "    # Adjust the range as necessary, starting from 2 since here we need timestep t=i-1 and i-2 in the data processing function\n",
    "    # args_for_processing = [(i, nc_files_in) for i in range(2, len(nc_files_in))]\n",
    "    args_for_processing = [(i, nc_files_in, lat, lon, 'mli', 'mlo', 'mlexpand') for i in range(2, 32)] # will create new input files with .mlexpand.\n",
    "\n",
    "    with mp.Pool(num_processes) as pool:\n",
    "        # Use pool.map to process files in parallel\n",
    "        pool.map(process_one_file, args_for_processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01ca2464-f92a-4374-99c7-3c95d2c6d903",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/homes/z/zeyuanhu/hugging/E3SM-MMF_ne4/train/0001-02/E3SM-MMF.mlexpand.0001-02-01-02400.nc\n",
      "/global/homes/z/zeyuanhu/hugging/E3SM-MMF_ne4/train/0001-02/E3SM-MMF.mlexpand.0001-02-01-03600.nc\n",
      "/global/homes/z/zeyuanhu/hugging/E3SM-MMF_ne4/train/0001-02/E3SM-MMF.mlexpand.0001-02-01-04800.nc\n",
      "/global/homes/z/zeyuanhu/hugging/E3SM-MMF_ne4/train/0001-02/E3SM-MMF.mlexpand.0001-02-01-06000.nc\n",
      "/global/homes/z/zeyuanhu/hugging/E3SM-MMF_ne4/train/0001-02/E3SM-MMF.mlexpand.0001-02-01-07200.nc\n"
     ]
    }
   ],
   "source": [
    "%ls /global/homes/z/zeyuanhu/hugging/E3SM-MMF_ne4/train/0001-02/*mlexpand*.nc | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471cb017-bdce-440e-8e5b-4c35e1aea428",
   "metadata": {},
   "source": [
    "## What does the process_one_file function do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e86f0-48c9-438a-b552-ef838b850d92",
   "metadata": {},
   "source": [
    "We had to put the process_one_file function in a separate .py file to let the multiprocessing function to work without problem. We copied the process_one_file function in climsim_adding_input.py below for your convenience to check what is inside the process_one_file function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791903b4-3589-44f6-994d-5e6f405b1eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_one_file_copy(args):\n",
    "    \"\"\"\n",
    "    Process a single NetCDF file by updating its dataset with information from previous files.\n",
    "    \n",
    "    Args:\n",
    "        i: int\n",
    "            The index of the current file in the full file list.\n",
    "        nc_files_in: list of str\n",
    "            List of the full filenames.\n",
    "        lat: xarray.DataArray\n",
    "            DataArray of latitude.\n",
    "        lon: xarray.DataArray\n",
    "            DataArray of longitude.\n",
    "        input_abbrev: str\n",
    "            The input file name abbreviation, the default input data should be 'mli'.\n",
    "        output_abbrev: str\n",
    "            The output file name abbreviation, the default output data should be 'mlo'.\n",
    "        input_abbrev_new: str\n",
    "            The abbreviation for the new input file name.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    i, nc_files_in, lat, lon, input_abbrev, output_abbrev, input_abbrev_new = args\n",
    "    dsin = xr.open_dataset(nc_files_in[i])\n",
    "    dsin_prev = xr.open_dataset(nc_files_in[i-1])\n",
    "    dsin_prev2 = xr.open_dataset(nc_files_in[i-2])\n",
    "    dsout_prev = xr.open_dataset(nc_files_in[i-1].replace(input_abbrev, output_abbrev))\n",
    "    dsout_prev2 = xr.open_dataset(nc_files_in[i-2].replace(input_abbrev, output_abbrev))\n",
    "    dsin['tm_state_t'] = dsin_prev['state_t']\n",
    "    dsin['tm_state_q0001'] = dsin_prev['state_q0001']\n",
    "    dsin['tm_state_q0002'] = dsin_prev['state_q0002']\n",
    "    dsin['tm_state_q0003'] = dsin_prev['state_q0003']\n",
    "    dsin['tm_state_u'] = dsin_prev['state_u']\n",
    "    dsin['tm_state_v'] = dsin_prev['state_v']\n",
    "\n",
    "    dsin['state_t_prvphy'] = (dsout_prev['state_t'] - dsin_prev['state_t'])/1200.\n",
    "    dsin['state_q0001_prvphy'] = (dsout_prev['state_q0001'] - dsin_prev['state_q0001'])/1200.\n",
    "    dsin['state_q0002_prvphy'] = (dsout_prev['state_q0002'] - dsin_prev['state_q0002'])/1200.\n",
    "    dsin['state_q0003_prvphy'] = (dsout_prev['state_q0003'] - dsin_prev['state_q0003'])/1200.\n",
    "    dsin['state_u_prvphy'] = (dsout_prev['state_u'] - dsin_prev['state_u'])/1200.\n",
    "\n",
    "    dsin['tm_state_t_prvphy'] = (dsout_prev2['state_t'] - dsin_prev2['state_t'])/1200.\n",
    "    dsin['tm_state_q0001_prvphy'] = (dsout_prev2['state_q0001'] - dsin_prev2['state_q0001'])/1200.\n",
    "    dsin['tm_state_q0002_prvphy'] = (dsout_prev2['state_q0002'] - dsin_prev2['state_q0002'])/1200.\n",
    "    dsin['tm_state_q0003_prvphy'] = (dsout_prev2['state_q0003'] - dsin_prev2['state_q0003'])/1200.\n",
    "    dsin['tm_state_u_prvphy'] = (dsout_prev2['state_u'] - dsin_prev2['state_u'])/1200.\n",
    "\n",
    "    dsin['state_t_dyn'] = (dsin['state_t'] - dsout_prev['state_t'])/1200.\n",
    "    dsin['state_q0_dyn'] = (dsin['state_q0001'] - dsout_prev['state_q0001'] + dsin['state_q0002'] - dsout_prev['state_q0002'] + dsin['state_q0003'] - dsout_prev['state_q0003'])/1200.\n",
    "    dsin['state_u_dyn'] = (dsin['state_u'] - dsout_prev['state_u'])/1200.\n",
    "\n",
    "    dsin['tm_state_t_dyn'] = (dsin_prev['state_t'] - dsout_prev2['state_t'])/1200.\n",
    "    dsin['tm_state_q0_dyn'] = (dsin_prev['state_q0001'] - dsout_prev2['state_q0001'] + dsin_prev['state_q0002'] - dsout_prev2['state_q0002'] + dsin_prev['state_q0003'] - dsout_prev2['state_q0003'])/1200.\n",
    "    dsin['tm_state_u_dyn'] = (dsin_prev['state_u'] - dsout_prev2['state_u'])/1200.\n",
    "\n",
    "    dsin['tm_state_ps'] = dsin_prev['state_ps']\n",
    "    dsin['tm_pbuf_SOLIN'] = dsin_prev['pbuf_SOLIN']\n",
    "    dsin['tm_pbuf_SHFLX'] = dsin_prev['pbuf_SHFLX']\n",
    "    dsin['tm_pbuf_LHFLX'] = dsin_prev['pbuf_LHFLX']\n",
    "    dsin['tm_pbuf_COSZRS'] = dsin_prev['pbuf_COSZRS']\n",
    "\n",
    "    dsin['lat'] = lat\n",
    "    dsin['lon'] = lon\n",
    "    clat = lat.copy()\n",
    "    slat = lat.copy()\n",
    "    icol = lat.copy()\n",
    "    clat[:] = np.cos(lat*2.*np.pi/360.)\n",
    "    slat[:] = np.sin(lat*2.*np.pi/360.)\n",
    "    icol[:] = np.arange(1,385)\n",
    "    dsin['clat'] = clat\n",
    "    dsin['slat'] = slat\n",
    "    dsin['icol'] = icol\n",
    "\n",
    "    new_file_path = nc_files_in[i].replace(input_abbrev, input_abbrev_new)\n",
    "    dsin.to_netcdf(new_file_path)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1274e9-23b1-4c96-9977-9214bfbbe324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climsim",
   "language": "python",
   "name": "climsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
