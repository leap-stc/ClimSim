{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6941e5c-270c-481b-bbfb-e319f3edf05b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-22 20:09:52.744061: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-22 20:09:52.744144: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-22 20:09:52.801154: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-22 20:09:52.932548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 20:09:55.088675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from climsim_utils.data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82db83-7ae0-423b-b994-7df5d734b101",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Instantiating class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b542b197-e371-4346-94b4-bc5ecfcf0f82",
   "metadata": {},
   "source": [
    "The example below will save training data in both .h5 and .npy format. Adjust if you only need one format. Also adjust input_abbrev to the input data files you will use. We expanded the original '.mli.' input files to include additional features such as previous steps' information, and '.mlexpand.' was just an arbitrary name we used for the expanded input files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826cf98d-4871-4a02-ba6a-fe90df706d5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Currently the training script would assume the training set is in .h5 format while the validation set is in .npy form. It's fine to only keep save_h5=True in the block below for generating training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4baee2-c25e-4e14-bae4-038e67a40740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid_path = '/global/u2/z/zeyuanhu/nvidia_codes/Climsim_private/grid_info/ClimSim_low-res_grid-info.nc'\n",
    "norm_path = '/global/u2/z/zeyuanhu/nvidia_codes/Climsim_private/preprocessing/normalizations/'\n",
    "\n",
    "grid_info = xr.open_dataset(grid_path)\n",
    "#no naming issue here. Here these normalization-related files are just placeholders since we set normalize=False in the data_utils.\n",
    "input_mean = xr.open_dataset(norm_path + 'inputs/input_mean_v5_pervar.nc')\n",
    "input_max = xr.open_dataset(norm_path + 'inputs/input_max_v5_pervar.nc')\n",
    "input_min = xr.open_dataset(norm_path + 'inputs/input_min_v5_pervar.nc')\n",
    "output_scale = xr.open_dataset(norm_path + 'outputs/output_scale_std_lowerthred_v5.nc')\n",
    "\n",
    "data = data_utils(grid_info = grid_info, \n",
    "                  input_mean = input_mean, \n",
    "                  input_max = input_max, \n",
    "                  input_min = input_min, \n",
    "                  output_scale = output_scale,\n",
    "                  input_abbrev = 'mlexpand',\n",
    "                  output_abbrev = 'mlo',\n",
    "                  normalize=False,\n",
    "                  save_h5=True,\n",
    "                  save_npy=True\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f1cf9ea-41d1-4b72-bff1-9a900188e834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set data path\n",
    "data.data_path = '/global/homes/z/zeyuanhu/scratch/hugging/E3SM-MMF_ne4/train/'\n",
    "\n",
    "# set inputs and outputs to V2 rh subset (rh means using RH to replace specific humidty in input feature)\n",
    "data.set_to_v2_rh_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53a7a139-d2f7-4229-8360-9f7f0422703e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_t',\n",
       " 'state_rh',\n",
       " 'state_q0002',\n",
       " 'state_q0003',\n",
       " 'state_u',\n",
       " 'state_v',\n",
       " 'pbuf_ozone',\n",
       " 'pbuf_CH4',\n",
       " 'pbuf_N2O',\n",
       " 'state_ps',\n",
       " 'pbuf_SOLIN',\n",
       " 'pbuf_LHFLX',\n",
       " 'pbuf_SHFLX',\n",
       " 'pbuf_TAUX',\n",
       " 'pbuf_TAUY',\n",
       " 'pbuf_COSZRS',\n",
       " 'cam_in_ALDIF',\n",
       " 'cam_in_ALDIR',\n",
       " 'cam_in_ASDIF',\n",
       " 'cam_in_ASDIR',\n",
       " 'cam_in_LWUP',\n",
       " 'cam_in_ICEFRAC',\n",
       " 'cam_in_LANDFRAC',\n",
       " 'cam_in_OCNFRAC',\n",
       " 'cam_in_SNOWHICE',\n",
       " 'cam_in_SNOWHLAND']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.input_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d01fa-eed6-493b-9e66-65b43796354b",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab985d2d-ce4b-4bfd-81cd-c67d9502a2fb",
   "metadata": {},
   "source": [
    "Below is an example of creating the training data by integrating the 7 year climsim simulation data. A subsampling of 1000 is used as an example. In the actual work we did, we used a stride_sample=1. We could not fit the full 7-year data into the memory wihout subsampling. If that's also the case for you, try to only process a subset of data at one time by adjusting regexps in set_regexps method. We saved 14 separate input .h5 files. For each year, we saved two files by setting start_idx=0 or 1. We have a folder like v2_full, which includes 14 subfolders named '11', '12', '21', '22', ..., '71','72', and each subfolder contains a train_input.h5 and train_target.h5. How you split to save training data won't influence the training. The training script will read in all the samples and randomly select samples across all the samples to form each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e07c633a-cad8-4cce-9f40-7f4acff845a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /global/homes/z/zeyuanhu/.conda/envs/climsim/lib/python3.10/site-packages/climsim_utils/data_utils.py:792: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From /global/homes/z/zeyuanhu/.conda/envs/climsim/lib/python3.10/site-packages/climsim_utils/data_utils.py:792: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 17:38:52.707705: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2256] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# set regular expressions for selecting training data\n",
    "data.set_regexps(data_split = 'train', \n",
    "                regexps = ['E3SM-MMF.mlexpand.000[1234567]-*-*-*.nc', # years 1 through 7\n",
    "                        'E3SM-MMF.mlexpand.0008-01-*-*.nc']) # first month of year 8\n",
    "# set temporal subsampling\n",
    "data.set_stride_sample(data_split = 'train', stride_sample = 1000)\n",
    "# create list of files to extract data from\n",
    "data.set_filelist(data_split = 'train', start_idx=0)\n",
    "# save numpy files of training data\n",
    "data.save_as_npy(data_split = 'train', save_path = '/global/homes/z/zeyuanhu/scratch/hugging/E3SM-MMF_ne4/preprocessing/v2_example/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc28f9-f333-4433-b9cc-8d0ecc3d7f07",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97cafa5c-0117-45e5-9488-0e2923f498f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set regular expressions for selecting validation data\n",
    "data.set_regexps(data_split = 'val',\n",
    "                 regexps = ['E3SM-MMF.mlexpand.0008-0[23456789]-*-*.nc', # months 2 through 9 of year 8\n",
    "                            'E3SM-MMF.mlexpand.0008-1[012]-*-*.nc', # months 10 through 12 of year 8\n",
    "                            'E3SM-MMF.mlexpand.0009-01-*-*.nc']) # first month of year 9\n",
    "# set temporal subsampling\n",
    "# data.set_stride_sample(data_split = 'val', stride_sample = 7)\n",
    "data.set_stride_sample(data_split = 'val', stride_sample = 700)\n",
    "# create list of files to extract data from\n",
    "data.set_filelist(data_split = 'val')\n",
    "# save numpy files of validation data\n",
    "data.save_as_npy(data_split = 'val', save_path = '/global/homes/z/zeyuanhu/scratch/hugging/E3SM-MMF_ne4/preprocessing/v2_example/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd7827-3210-444e-be21-9126518c3cc6",
   "metadata": {},
   "source": [
    "### Create test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c81c8b-486b-4fab-8167-24e55b4c7719",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.data_path = '/global/homes/z/zeyuanhu/scratch/hugging/E3SM-MMF_ne4/_test/'\n",
    "\n",
    "data.set_to_v4_vars()\n",
    "\n",
    "# set regular expressions for selecting validation data\n",
    "data.set_regexps(data_split = 'test',\n",
    "                 regexps = ['E3SM-MMF.mlexpand.0009-0[3456789]-*-*.nc', \n",
    "                            'E3SM-MMF.mlexpand.0009-1[012]-*-*.nc',\n",
    "                            'E3SM-MMF.mlexpand.0010-*-*-*.nc',\n",
    "                            'E3SM-MMF.mlexpand.0011-0[12]-*-*.nc'])\n",
    "# set temporal subsampling\n",
    "# data.set_stride_sample(data_split = 'test', stride_sample = 7)\n",
    "data.set_stride_sample(data_split = 'test', stride_sample = 700)\n",
    "# create list of files to extract data from\n",
    "data.set_filelist(data_split = 'test')\n",
    "# save numpy files of validation data\n",
    "data.save_as_npy(data_split = 'test', save_path = '/global/homes/z/zeyuanhu/scratch/hugging/E3SM-MMF_ne4/preprocessing/v2_example/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad0d01b8-b20c-4dec-a967-981f6ecf514b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_input.h5\ttest_target.npy  train_target.h5   val_input.npy\n",
      "test_input.npy\ttrain_input.h5\t train_target.npy  val_target.h5\n",
      "test_target.h5\ttrain_input.npy  val_input.h5\t   val_target.npy\n"
     ]
    }
   ],
   "source": [
    "!ls /global/homes/z/zeyuanhu/scratch/hugging/E3SM-MMF_ne4/preprocessing/v2_example/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climsim",
   "language": "python",
   "name": "climsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
