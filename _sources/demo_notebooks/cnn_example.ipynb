{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 18:17:02.868544: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-26 18:17:02.948491: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-26 18:17:02.949921: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-26 18:17:04.241559: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "try:\n",
    "    for kgpu in range(len(physical_devices)):\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[kgpu], True)\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input and output variable list\n",
    "- Note that ptend_t and ptend_q0001 are not in the output (mlo) netcdf files, but calculated real-time on a tf Dataset object.\n",
    "- Variable list: https://docs.google.com/spreadsheets/d/1ljRfHq6QB36u0TuoxQXcV4_DSQUR0X4UimZ4QHR8f9M/edit#gid=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in/out variable lists\n",
    "cfg = dict()\n",
    "cfg['vars_mli'] = ['state_t','state_q0001','state_ps','pbuf_SOLIN', 'pbuf_LHFLX', 'pbuf_SHFLX']\n",
    "cfg['vars_mlo'] = ['ptend_t','ptend_q0001','cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC','cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tf dataset\n",
    "- ref: https://www.noahbrenowitz.com/post/loading_netcdfs/\n",
    "- ref: https://www.tensorflow.org/api_docs/python/tf/data/Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E3SMDataset(object):\n",
    "    def __init__(self, cfg, verbose=False):\n",
    "        '''\n",
    "        '''\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Initialize normalization\n",
    "        self.mli_mean = xr.open_dataset('./norm_factors/mli_mean.nc')\n",
    "        self.mli_min = xr.open_dataset('./norm_factors/mli_min.nc')\n",
    "        self.mli_max = xr.open_dataset('./norm_factors/mli_max.nc')\n",
    "        self.mlo_scale = xr.open_dataset('./norm_factors/mlo_scale.nc')\n",
    "\n",
    "        # Init in/out variable list\n",
    "        self.vars_mli = cfg['vars_mli']\n",
    "        self.vars_mlo = cfg['vars_mlo']\n",
    "\n",
    "        # Used to define in- output shape\n",
    "        self.in_out_shape = self.cfg['in_out_shape']\n",
    "        \n",
    "    def flatten_layers_and_vars_1d(self, ds, dso):\n",
    "        '''\n",
    "        Concatenates all variables into (batch,124) and (batch,128) vectors. Used for FCNN input\n",
    "        '''\n",
    "        # stack\n",
    "        #ds = ds.stack({'batch':{'sample','ncol'}})\n",
    "        ds = ds.stack({'batch':{'ncol'}})\n",
    "        ds = ds.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mli')\n",
    "        #dso = dso.stack({'batch':{'sample','ncol'}})\n",
    "        dso = dso.stack({'batch':{'ncol'}})\n",
    "        dso = dso.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mlo')\n",
    "        return ds, dso\n",
    "        \n",
    "    def pad_and_stack_layers_and_vars_1d(self, ds, dso):\n",
    "        '''\n",
    "        Pads and stack all variables into (batch, n_vertical_levels, n_variables), \n",
    "        e.g., input: (batch, 60, 6) and output: (batch, 60, 10)\n",
    "        Args:\n",
    "            ds xarray.Dataset(lev, ncol) with vars_mli of shapes (lev, ncol) and (ncol)\n",
    "            dso xarray.Dataset(lev, ncol) with vars_mlo of shapes (lev, ncol) and (ncol)\n",
    "        Returns:\n",
    "            arr xarray.DataArray(batch, lev, variable)\n",
    "            arro xarray.DataArray(batch, lev, variable)\n",
    "        '''\n",
    "        ds = ds.stack({'batch':{'ncol'}})\n",
    "        (ds,) = xr.broadcast(ds) # repeat global variables across levels\n",
    "        arr = ds.to_array('mlvar', name='mli')\n",
    "        arr = arr.transpose('batch', 'lev', 'mlvar') \n",
    "\n",
    "        dso = dso.stack({'batch':{'ncol'}})\n",
    "        (dso,) = xr.broadcast(dso)\n",
    "        arro = dso.to_array('mlvar', name='mlo')\n",
    "        arro = arro.transpose('batch', 'lev', 'mlvar')\n",
    "\n",
    "        return arr, arro\n",
    "\n",
    "    def load_nc_dir_with_generator(self, filelist:list):\n",
    "        '''\n",
    "        Return tf.dataset        \n",
    "        '''\n",
    "        # Define in and output shape\n",
    "        self.ncol = xr.open_dataset(f_mli[0], engine='netcdf4').dims['ncol']\n",
    "        self.lev = xr.open_dataset(f_mli[0], engine='netcdf4').dims['lev']\n",
    "        if self.cfg['in_out_shape'] == 'pad_and_stack_layers_and_vars_1d':\n",
    "            self.in_shape = (None, self.lev, len(self.vars_mli))\n",
    "            self.out_shape = (None, self.lev, len(self.vars_mlo))\n",
    "        else:\n",
    "            self.in_shape = (None,124)\n",
    "            self.out_shape = (None,128)\n",
    "\n",
    "        # Create generator function\n",
    "        def gen():\n",
    "            for file in filelist:\n",
    "                # read mli\n",
    "                ds = xr.open_dataset(file, engine='netcdf4')\n",
    "                ds = ds[self.vars_mli]\n",
    "                \n",
    "                # read mlo\n",
    "                dso = xr.open_dataset(file.replace('.mli.','.mlo.'), engine='netcdf4')\n",
    "                \n",
    "                # make mlo variales: ptend_t and ptend_q0001\n",
    "                dso['ptend_t'] = (dso['state_t'] - ds['state_t'])/1200 # T tendency [K/s]\n",
    "                dso['ptend_q0001'] = (dso['state_q0001'] - ds['state_q0001'])/1200 # Q tendency [kg/kg/s]\n",
    "                dso = dso[self.vars_mlo]\n",
    "                \n",
    "                # normalization, scaling\n",
    "                ds = (ds-self.mli_mean)/(self.mli_max-self.mli_min)\n",
    "                dso = dso*self.mlo_scale\n",
    "    \n",
    "                # stack\n",
    "                if self.cfg['in_out_shape'] == 'pad_and_stack_layers_and_vars_1d':\n",
    "                    ds, dso = self.pad_and_stack_layers_and_vars_1d(ds, dso)\n",
    "                else:\n",
    "                    ds, dso = self.flatten_layers_and_vars_1d(ds, dso)\n",
    "\n",
    "                yield (ds.values, dso.values)\n",
    "    \n",
    "        return tf.data.Dataset.from_generator(\n",
    "            gen,\n",
    "            output_types=(tf.float64, tf.float64),\n",
    "            output_shapes=(self.in_shape,self.out_shape)\n",
    "        )\n",
    "\n",
    "# e3smdataset = E3SMDataset(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate tf.data.Dataset object here\n",
    "- Dataset file size and dimensions: https://docs.google.com/document/d/1HgfZZJM0SygjWvSAJ5kSfql9aXUFkvLybL36p-vmdZc/edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of train, val filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def get_filenames(dataroot, filenames, stride_sample=1):\n",
    "    '''\n",
    "    Create list of filenames\n",
    "    \n",
    "    Args:\n",
    "        dataroot: Relative or global path to directory of filenames\n",
    "        filenames: List of filename wildcards\n",
    "        stride_sample int: pick every nth sample\n",
    "    '''\n",
    "    dataroot = Path(dataroot)\n",
    "    \n",
    "    filepaths = []\n",
    "    for filename in filenames:\n",
    "        filepaths.extend(glob.glob(str(cfg['dataroot']/Path(filename))))\n",
    "    # f_mli = sorted([*f_mli1, *f_mli2]) # I commented this out. It seems unecessary to sort the list if it will be shuffled\n",
    "    random.shuffle(filepaths)\n",
    "    filepaths = filepaths[::stride_sample]\n",
    "    # f_mli = f_mli[0:72*5] # debugging\n",
    "    # random.shuffle(f_mli) # I commented this out. It seems unnecessary to shuffle twice.\n",
    "\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['stride_sample'] = 1 # every nth sample\n",
    "cfg['dataroot'] = Path('data/micro_sample/')\n",
    "\n",
    "# todo - ritwik: make sure correct train/val/test split is used.\n",
    "cfg['filenames_train'] = ['E3SM-MMF.mli.000[1234567]-*-*-*.nc', 'E3SM-MMF.mli.0008-01-*-*.nc']\n",
    "cfg['filenames_val'] = ['E3SM-MMF.mli.0008-0[23456789]-*-*.nc', 'E3SM-MMF.mli.0008-1[012]-*-*.nc', 'E3SM-MMF.mli.0009-01-*-*.nc']\n",
    "\n",
    "f_mli = get_filenames(cfg['dataroot'], cfg['filenames_train'], stride_sample=1)\n",
    "f_mli_val = get_filenames(cfg['dataroot'], cfg['filenames_val'], stride_sample=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### instantiate train, val loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define in-out-shape depending on which model is used:\n",
    "# cfg['in_out_shape'] = 'flatten_layers_and_vars_1d' # use this for fcnn\n",
    "cfg['in_out_shape'] = 'pad_and_stack_layers_and_vars_1d' # use this for cnn\n",
    "e3smdataset = E3SMDataset(cfg)\n",
    "\n",
    "ncol = xr.open_dataset(f_mli[0], engine='netcdf4').dims['ncol']\n",
    "shuffle_buffer = 12*ncol # todo: where does 12 come from?\n",
    "cfg['batch_size'] = int(float(ncol) / 4) # e.g., 96. Todo: where does 4 come from? Presumably number of GPUs\n",
    "\n",
    "# Todo - ritwik: double check if unbatch, shuffle, and prefetch here make sense. I checked them and couldn't find anything wrong with this, but i don't fully understand how the dataset should be shuffled if it's not randomly in columns.\n",
    "tds = e3smdataset.load_nc_dir_with_generator(f_mli)\n",
    "tds = tds.unbatch()\n",
    "tds = tds.shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=True)\n",
    "tds = tds.batch(cfg['batch_size'])\n",
    "tds = tds.prefetch(buffer_size=int(shuffle_buffer/ncol)) # in relation to the batch size\n",
    "\n",
    "tds_val = e3smdataset.load_nc_dir_with_generator(f_mli_val)\n",
    "tds_val = tds_val.unbatch()\n",
    "tds_val = tds_val.shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=True)\n",
    "tds_val = tds_val.batch(cfg['batch_size'])\n",
    "tds_val = tds_val.prefetch(buffer_size=int(shuffle_buffer/ncol))\n",
    "\n",
    "#list(tds)\n",
    "# for count_batch in tds.repeat().batch(10).take(1):\n",
    "#     print(count_batch[0].numpy())\n",
    "#count_batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot some data statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Total # of input files: 10\n",
      "[TRAIN] Total # of samples (nfiles * ncols): 3840\n",
      "[VAL] Total # of input files: 0\n",
      "[VAL] Total # of samples (nfiles * ncols): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 18:17:08.059325: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-26 18:17:08.061563: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: \t(96, 60, 6)\n",
      "Output shape: \t(96, 60, 10)\n",
      "Range of vertically resolved vars: [-1.0254011839104458, 2.1109701538780046]\n",
      "Range of global vars: [0.0, 2.3392385424469184]\n"
     ]
    }
   ],
   "source": [
    "print(f'[TRAIN] Total # of input files: {len(f_mli)}')\n",
    "print(f'[TRAIN] Total # of samples (nfiles * ncols): {len(f_mli)*384}')\n",
    "print(f'[VAL] Total # of input files: {len(f_mli_val)}')\n",
    "print(f'[VAL] Total # of samples (nfiles * ncols): {len(f_mli_val)*384}')\n",
    "\n",
    "input, output = next(iter(tds.take(1)))\n",
    "print(f'Input shape: \\t{input.shape}') \n",
    "print(f'Output shape: \\t{output.shape}')\n",
    "\n",
    "print(f'Range of vertically resolved vars: [{tf.reduce_min(output[...,:3])}, {tf.reduce_max(output[...,:3])}]')\n",
    "print(f'Range of global vars: [{tf.reduce_min(output[...,3:])}, {tf.reduce_max(output[...,3:])}]')\n",
    "if tf.reduce_min(output[...,3:]) < 0:\n",
    "    print('[WARNING] global vars are assumed to be positive in CNN model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML training\n",
    "- While 4 GPUs are available on the node, using multi GPUs (with 'tf.distribute.MirroredStrategy()' strategy) does not speed up training process. It is possibly due to that the current Dataset pipeline is sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Emulator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 124)]        0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          64000       ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          262656      ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          65664       ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 120)          15480       ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 8)            1032        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 128)          0           ['dense_3[0][0]',                \n",
      "                                                                  'dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 408,832\n",
      "Trainable params: 408,832\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define FCNN\n",
    "\n",
    "# strategy = tf.distribute.MirroredStrategy()\n",
    "# with strategy.scope():\n",
    "\n",
    "# model params\n",
    "input_length = 2*60 + 4\n",
    "output_length_lin  = 2*60\n",
    "output_length_relu = 8\n",
    "output_length = output_length_lin + output_length_relu\n",
    "n_nodes = 512\n",
    "\n",
    "# constrcut a model\n",
    "input_layer    = keras.layers.Input(shape=(input_length,), name='input')\n",
    "hidden_0       = keras.layers.Dense(n_nodes, activation='relu')(input_layer)\n",
    "hidden_1       = keras.layers.Dense(n_nodes, activation='relu')(hidden_0)\n",
    "output_pre     = keras.layers.Dense(output_length, activation='elu')(hidden_1)\n",
    "output_lin     = keras.layers.Dense(output_length_lin,activation='linear')(output_pre)\n",
    "output_relu    = keras.layers.Dense(output_length_relu,activation='relu')(output_pre)\n",
    "output_layer   = keras.layers.Concatenate()([output_lin, output_relu])\n",
    "\n",
    "model = keras.Model(input_layer, output_layer, name='Emulator')\n",
    "model.summary()\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer=keras.optimizers.Adam(), #optimizer=keras.optimizers.Adam(learning_rate=clr),\n",
    "              loss='mse',\n",
    "              metrics=['mse','mae','accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "def init_cnn(in_shape, out_shape,\n",
    "            n_vertical_vars=2,\n",
    "            channel_dims=[32, 32],\n",
    "            kernels=[3,3],\n",
    "            activation='gelu',\n",
    "            pre_out_activation='elu',\n",
    "            norm=None,\n",
    "            dropout=0.0):\n",
    "    '''\n",
    "    Create a ResNet-style 1D CNN. The data is of shape (batch, lev, vars) \n",
    "    where lev is treated as the spatial dimension. The architecture \n",
    "    consists of residual blocks with each two conv layers.\n",
    "    '''\n",
    "    # Define output shapes\n",
    "    output_length_lin  = n_vertical_vars \n",
    "    output_length_relu = out_shape[-1]-n_vertical_vars\n",
    "\n",
    "    # Initialize special layers\n",
    "    norm_layer = get_normalization_layer(norm)\n",
    "    if len(channel_dims) != len(kernels):\n",
    "        print(f'[WARNING] Length of channel_dims and kernels does not match. Using 1st argument in kernels, {kernels[0]}, for every layer')\n",
    "        kernels = [kernels[0]] * len(channel_dims)\n",
    "\n",
    "    # Initialize model architecture\n",
    "    input_layer = keras.Input(shape=in_shape)\n",
    "    x = input_layer # Set aside input layer\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "    for filters, kernel_size in zip(channel_dims, kernels):\n",
    "        # First conv layer in block\n",
    "        # 'same' applies zero padding.\n",
    "        x = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "        # todo: add se_block\n",
    "        if norm_layer:\n",
    "            x = norm_layer(x)\n",
    "        x = keras.layers.Activation(activation)(x)\n",
    "        x = keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "        # Second convolution layer\n",
    "        x = Conv1D(filters=filters, kernel_size=kernel_size, padding='same')(x)\n",
    "        if norm_layer:\n",
    "            x = norm_layer(x)\n",
    "        x = keras.layers.Activation(activation)(x)\n",
    "        x = keras.layers.Dropout(dropout)(x)\n",
    "        \n",
    "        # Project residual\n",
    "        residual = Conv1D(filters=filters, kernel_size=1, strides=1, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = keras.layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Output layers.\n",
    "    # x = keras.layers.Dense(filters[-1], activation='gelu')(x) # Add another last layer.\n",
    "    x = Conv1D(out_shape[-1], kernel_size=1, activation=pre_out_activation, padding=\"same\")(x)\n",
    "    # Assume that vertically resolved variables follow no particular range.\n",
    "    output_lin     = keras.layers.Dense(output_length_lin,activation='linear')(x)\n",
    "    # Assume that all globally resolved variables are positive.\n",
    "    output_relu    = keras.layers.Dense(output_length_relu,activation='relu')(x)\n",
    "    output_layer   = keras.layers.Concatenate()([output_lin, output_relu])\n",
    "    \n",
    "    model = keras.Model(input_layer, output_layer, name='cnn')    \n",
    "    return model\n",
    "\n",
    "def get_normalization_layer(norm=None, axis=[1,2]):\n",
    "    '''\n",
    "    Return normalization layer given string\n",
    "    Args:\n",
    "        norm string\n",
    "        axis indices for layer normalization. todo: don't hard-code\n",
    "    '''\n",
    "    if norm == 'layer_norm':\n",
    "        norm_layer = tf.keras.layers.LayerNormalization(axis=axis)\n",
    "    elif norm == 'batch_norm':\n",
    "        norm_layer = tf.keras.layers.BatchNormalization()\n",
    "    else:\n",
    "        norm_layer = None\n",
    "    return norm_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] Length of channel_dims and kernels does not match. Using 1st argument in kernels, 5, for every layer\n",
      "Model: \"cnn\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 60, 6)]      0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 60, 32)       992         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 60, 32)       0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 60, 32)       0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 60, 32)       5152        ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 60, 32)       0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 60, 32)       0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 60, 32)       224         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 60, 32)       0           ['dropout_1[0][0]',              \n",
      "                                                                  'conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 60, 64)       10304       ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 60, 64)       0           ['conv1d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 60, 64)       0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 60, 64)       20544       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 60, 64)       0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 60, 64)       0           ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 60, 64)       2112        ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 60, 64)       0           ['dropout_3[0][0]',              \n",
      "                                                                  'conv1d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 60, 128)      41088       ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 60, 128)      0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 60, 128)      0           ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 60, 128)      82048       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 60, 128)      0           ['conv1d_7[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 60, 128)      0           ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 60, 128)      8320        ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 60, 128)      0           ['dropout_5[0][0]',              \n",
      "                                                                  'conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 60, 256)      164096      ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 60, 256)      0           ['conv1d_9[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 60, 256)      0           ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 60, 256)      327936      ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 60, 256)      0           ['conv1d_10[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 60, 256)      0           ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 60, 256)      33024       ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 60, 256)      0           ['dropout_7[0][0]',              \n",
      "                                                                  'conv1d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv1d_12 (Conv1D)             (None, 60, 10)       2570        ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 60, 2)        22          ['conv1d_12[0][0]']              \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 60, 8)        88          ['conv1d_12[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 60, 10)       0           ['dense_5[0][0]',                \n",
      "                                                                  'dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 698,520\n",
      "Trainable params: 698,520\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cfg['in_shape'] = e3smdataset.in_shape[1:] # e.g., 60,6\n",
    "cfg['out_shape'] = e3smdataset.out_shape[1:] # e.g., 60, 10\n",
    "cfg['n_vertical_vars'] = 2 # Number of vertically resolved variables. Todo get rid of hardcoding. \n",
    "cfg['channel_dims'] = [32, 64, 128, 256] # Number of features in each convolutional block. Length determines number of blocks.\n",
    "cfg['kernels'] = [5] # kernel_size in each convolutional block. Is either of same length as channel_dims or first entry is used for every layer.\n",
    "cfg['activation'] = 'gelu' # Activation for all internal layers\n",
    "cfg['pre_out_activation'] = 'elu' # Activation right before output activation. Not sure why fcnn used elu and not gelu here.\n",
    "cfg['loss'] = 'mse'\n",
    "cfg['lr'] = 0.001\n",
    "cfg['epsilon'] = 1e-7 # Small constant in adam for numerical stability. Default value is 1e-07. But keras mentions that other papers use 1.0 or 0.1\n",
    "cfg['weight_decay'] = 1e-6\n",
    "cfg['norm'] = None # Can use 'layer_norm', 'batch_norm', None\n",
    "cfg['dropout'] = 0.0 # Dropout probability in range [0,1]\n",
    "cfg['verbose'] = True\n",
    "\n",
    "model = init_cnn(in_shape=cfg['in_shape'],\n",
    "        out_shape=cfg['out_shape'],\n",
    "        n_vertical_vars=cfg['n_vertical_vars'],\n",
    "        channel_dims=cfg['channel_dims'],\n",
    "        kernels=cfg['kernels'],\n",
    "        activation=cfg['activation'],\n",
    "        norm=cfg['norm'],\n",
    "        dropout=cfg['dropout']\n",
    ")\n",
    "if cfg['verbose']:\n",
    "    model.summary()\n",
    "\n",
    "# todo add lr scheduler.\n",
    "# clr = tfa.optimizers.CyclicalLearningRate(initial_learning_rate=INIT_LR,\n",
    "#                                      maximal_learning_rate=MAX_LR,\n",
    "#                                      scale_fn=lambda x: 1/(2.**(x-1)),\n",
    "#                                       step_size= 2 * steps_per_epoch,\n",
    "#                                       scale_mode = 'cycle'\n",
    "#                                      )\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=cfg['lr'],\n",
    "    epsilon=cfg['epsilon'],\n",
    "    weight_decay=cfg['weight_decay'])\n",
    "\n",
    "loss = cfg['loss']\n",
    "\n",
    "model.compile(optimizer=optimizer, #\n",
    "              loss=loss,\n",
    "              metrics=['mse','mae','accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "# a. tensorboard\n",
    "tboard_callback = keras.callbacks.TensorBoard(log_dir = './logs_tensorboard',\n",
    "                                              histogram_freq = 1,)\n",
    "\n",
    "# b. checkpoint\n",
    "filepath_checkpoint = 'saved_model/best_model_proto.h5'\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=filepath_checkpoint,\n",
    "                                                            save_weights_only=False,\n",
    "                                                            monitor='val_mse',\n",
    "                                                            mode='min',\n",
    "                                                            save_best_only=True)\n",
    "\n",
    "# c. csv logger\n",
    "filepath_csv = 'csv_logger.txt'\n",
    "csv_callback = keras.callbacks.CSVLogger(filepath_csv, separator=\",\", append=True)\n",
    "\n",
    "my_callbacks= [tboard_callback, checkpoint_callback, csv_callback]\n",
    "\n",
    "# !mkdir logs_tensorboard\n",
    "# !mkdir saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 18:17:12.136165: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-26 18:17:12.137648: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     40/Unknown - 27s 491ms/step - loss: 0.1615 - mse: 0.1615 - mae: 0.1982 - accuracy: 0.7801"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 18:17:39.919740: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-05-26 18:17:39.920500: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 29s 542ms/step - loss: 0.1615 - mse: 0.1615 - mae: 0.1982 - accuracy: 0.7801\n",
      "Epoch 2/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0552 - mse: 0.0552 - mae: 0.1205 - accuracy: 0.9368WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 17s 415ms/step - loss: 0.0552 - mse: 0.0552 - mae: 0.1205 - accuracy: 0.9368\n",
      "Epoch 3/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0471 - mse: 0.0471 - mae: 0.1050 - accuracy: 0.9433WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 17s 422ms/step - loss: 0.0471 - mse: 0.0471 - mae: 0.1050 - accuracy: 0.9433\n",
      "Epoch 4/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0449 - mse: 0.0449 - mae: 0.1025 - accuracy: 0.9452WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 18s 430ms/step - loss: 0.0449 - mse: 0.0449 - mae: 0.1025 - accuracy: 0.9452\n",
      "Epoch 5/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0450 - mse: 0.0450 - mae: 0.1032 - accuracy: 0.9467WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 23s 577ms/step - loss: 0.0450 - mse: 0.0450 - mae: 0.1032 - accuracy: 0.9467\n",
      "Epoch 6/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0435 - mse: 0.0435 - mae: 0.1010 - accuracy: 0.9454WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 18s 432ms/step - loss: 0.0435 - mse: 0.0435 - mae: 0.1010 - accuracy: 0.9454\n",
      "Epoch 7/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0417 - mse: 0.0417 - mae: 0.0978 - accuracy: 0.9492WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 17s 402ms/step - loss: 0.0417 - mse: 0.0417 - mae: 0.0978 - accuracy: 0.9492\n",
      "Epoch 8/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0414 - mse: 0.0414 - mae: 0.0992 - accuracy: 0.9480WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 16s 388ms/step - loss: 0.0414 - mse: 0.0414 - mae: 0.0992 - accuracy: 0.9480\n",
      "Epoch 9/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0390 - mse: 0.0390 - mae: 0.0944 - accuracy: 0.9506WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 16s 396ms/step - loss: 0.0390 - mse: 0.0390 - mae: 0.0944 - accuracy: 0.9506\n",
      "Epoch 10/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0393 - mse: 0.0393 - mae: 0.0949 - accuracy: 0.9521WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 16s 383ms/step - loss: 0.0393 - mse: 0.0393 - mae: 0.0949 - accuracy: 0.9521\n",
      "Epoch 11/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0384 - mse: 0.0384 - mae: 0.0939 - accuracy: 0.9467WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 17s 409ms/step - loss: 0.0384 - mse: 0.0384 - mae: 0.0939 - accuracy: 0.9467\n",
      "Epoch 12/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0375 - mse: 0.0375 - mae: 0.0930 - accuracy: 0.9483WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 17s 412ms/step - loss: 0.0375 - mse: 0.0375 - mae: 0.0930 - accuracy: 0.9483\n",
      "Epoch 13/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0371 - mse: 0.0371 - mae: 0.0928 - accuracy: 0.9500WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 16s 395ms/step - loss: 0.0371 - mse: 0.0371 - mae: 0.0928 - accuracy: 0.9500\n",
      "Epoch 14/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0364 - mse: 0.0364 - mae: 0.0913 - accuracy: 0.9523WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 17s 421ms/step - loss: 0.0364 - mse: 0.0364 - mae: 0.0913 - accuracy: 0.9523\n",
      "Epoch 15/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0352 - mse: 0.0352 - mae: 0.0899 - accuracy: 0.9508WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 17s 424ms/step - loss: 0.0352 - mse: 0.0352 - mae: 0.0899 - accuracy: 0.9508\n",
      "Epoch 16/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0338 - mse: 0.0338 - mae: 0.0867 - accuracy: 0.9521WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 19s 469ms/step - loss: 0.0338 - mse: 0.0338 - mae: 0.0867 - accuracy: 0.9521\n",
      "Epoch 17/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0337 - mse: 0.0337 - mae: 0.0877 - accuracy: 0.9538WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 29s 715ms/step - loss: 0.0337 - mse: 0.0337 - mae: 0.0877 - accuracy: 0.9538\n",
      "Epoch 18/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0331 - mse: 0.0331 - mae: 0.0862 - accuracy: 0.9530WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 22s 509ms/step - loss: 0.0331 - mse: 0.0331 - mae: 0.0862 - accuracy: 0.9530\n",
      "Epoch 19/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0315 - mse: 0.0315 - mae: 0.0837 - accuracy: 0.9536WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 20s 475ms/step - loss: 0.0315 - mse: 0.0315 - mae: 0.0837 - accuracy: 0.9536\n",
      "Epoch 20/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0308 - mse: 0.0308 - mae: 0.0820 - accuracy: 0.9565WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 22s 532ms/step - loss: 0.0308 - mse: 0.0308 - mae: 0.0820 - accuracy: 0.9565\n",
      "Epoch 21/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0308 - mse: 0.0308 - mae: 0.0823 - accuracy: 0.9542WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 27s 615ms/step - loss: 0.0308 - mse: 0.0308 - mae: 0.0823 - accuracy: 0.9542\n",
      "Epoch 22/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0297 - mse: 0.0297 - mae: 0.0807 - accuracy: 0.9579WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 25s 595ms/step - loss: 0.0297 - mse: 0.0297 - mae: 0.0807 - accuracy: 0.9579\n",
      "Epoch 23/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0299 - mse: 0.0299 - mae: 0.0820 - accuracy: 0.9513WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 20s 466ms/step - loss: 0.0299 - mse: 0.0299 - mae: 0.0820 - accuracy: 0.9513\n",
      "Epoch 24/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0293 - mse: 0.0293 - mae: 0.0803 - accuracy: 0.9557WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 20s 479ms/step - loss: 0.0293 - mse: 0.0293 - mae: 0.0803 - accuracy: 0.9557\n",
      "Epoch 25/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0289 - mse: 0.0289 - mae: 0.0796 - accuracy: 0.9570WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 15s 367ms/step - loss: 0.0289 - mse: 0.0289 - mae: 0.0796 - accuracy: 0.9570\n",
      "Epoch 26/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0292 - mse: 0.0292 - mae: 0.0806 - accuracy: 0.9542WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 16s 375ms/step - loss: 0.0292 - mse: 0.0292 - mae: 0.0806 - accuracy: 0.9542\n",
      "Epoch 27/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0283 - mse: 0.0283 - mae: 0.0785 - accuracy: 0.9541WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 19s 461ms/step - loss: 0.0283 - mse: 0.0283 - mae: 0.0785 - accuracy: 0.9541\n",
      "Epoch 28/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0276 - mse: 0.0276 - mae: 0.0771 - accuracy: 0.9586WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 21s 493ms/step - loss: 0.0276 - mse: 0.0276 - mae: 0.0771 - accuracy: 0.9586\n",
      "Epoch 29/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0236 - mse: 0.0236 - mae: 0.0750 - accuracy: 0.9577WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 25s 618ms/step - loss: 0.0236 - mse: 0.0236 - mae: 0.0750 - accuracy: 0.9577\n",
      "Epoch 30/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0201 - mse: 0.0201 - mae: 0.0712 - accuracy: 0.9565WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 27s 632ms/step - loss: 0.0201 - mse: 0.0201 - mae: 0.0712 - accuracy: 0.9565\n",
      "Epoch 31/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0186 - mse: 0.0186 - mae: 0.0685 - accuracy: 0.9568WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 22s 514ms/step - loss: 0.0186 - mse: 0.0186 - mae: 0.0685 - accuracy: 0.9568\n",
      "Epoch 32/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0175 - mse: 0.0175 - mae: 0.0668 - accuracy: 0.9577WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 25s 598ms/step - loss: 0.0175 - mse: 0.0175 - mae: 0.0668 - accuracy: 0.9577\n",
      "Epoch 33/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0173 - mse: 0.0173 - mae: 0.0658 - accuracy: 0.9582WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 24s 551ms/step - loss: 0.0173 - mse: 0.0173 - mae: 0.0658 - accuracy: 0.9582\n",
      "Epoch 34/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0167 - mse: 0.0167 - mae: 0.0646 - accuracy: 0.9601WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 25s 594ms/step - loss: 0.0167 - mse: 0.0167 - mae: 0.0646 - accuracy: 0.9601\n",
      "Epoch 35/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0171 - mse: 0.0171 - mae: 0.0650 - accuracy: 0.9544WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 23s 539ms/step - loss: 0.0171 - mse: 0.0171 - mae: 0.0650 - accuracy: 0.9544\n",
      "Epoch 36/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0166 - mse: 0.0166 - mae: 0.0647 - accuracy: 0.9610WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 21s 515ms/step - loss: 0.0166 - mse: 0.0166 - mae: 0.0647 - accuracy: 0.9610\n",
      "Epoch 37/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0156 - mse: 0.0156 - mae: 0.0620 - accuracy: 0.9609WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 22s 530ms/step - loss: 0.0156 - mse: 0.0156 - mae: 0.0620 - accuracy: 0.9609\n",
      "Epoch 38/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0161 - mse: 0.0161 - mae: 0.0630 - accuracy: 0.9610WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 21s 500ms/step - loss: 0.0161 - mse: 0.0161 - mae: 0.0630 - accuracy: 0.9610\n",
      "Epoch 39/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0149 - mse: 0.0149 - mae: 0.0609 - accuracy: 0.9602WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 20s 484ms/step - loss: 0.0149 - mse: 0.0149 - mae: 0.0609 - accuracy: 0.9602\n",
      "Epoch 40/40\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.0151 - mse: 0.0151 - mae: 0.0614 - accuracy: 0.9612WARNING:tensorflow:Can save best model only with val_mse available, skipping.\n",
      "40/40 [==============================] - 19s 442ms/step - loss: 0.0151 - mse: 0.0151 - mae: 0.0614 - accuracy: 0.9612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f471361dbe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg['n_epochs'] = 40\n",
    "\n",
    "model.fit(tds, \n",
    "          validation_data=tds_val,\n",
    "          callbacks=my_callbacks,\n",
    "          epochs=cfg['n_epochs'], \n",
    "          batch_size=cfg['batch_size'], \n",
    "          verbose=cfg['verbose'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e3sm",
   "language": "python",
   "name": "e3sm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
